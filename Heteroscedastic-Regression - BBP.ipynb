{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "tod = torch.distributions\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-scale likelihood, can add all ensemble or not\n",
    "# diagonal uniform variance is assumed for this multivariate normal dist\n",
    "\"Q1: It seems this one is messed up with 1 / multi dimensional gaussian form?\"\n",
    "\"is sigma  /output / target even k dimensional?\"\n",
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "\n",
    "\"Q2: what is weights? what does it represent?\"\n",
    "\"kl-divergence should takes integration over the entire probability space\"\n",
    "def get_kl_divergence(weights, prior, varpost):\n",
    "    prior_loglik = prior.loglik(weights)\n",
    "    varpost_loglik = varpost.loglik(weights)\n",
    "    varpost_lik = varpost_loglik.exp()\n",
    "    \n",
    "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
    "\n",
    "\n",
    "# this is standard Element-wise one-dimensional gaussian distribution\n",
    "\"This is a element-wise indepedent gaussian distributed (multivariate Gaussian) multivariate-distribution\"\n",
    "\"mu/sigma has same shape, output is summed over all the dimensions\"\n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \"not sure what scale/rho_init represents\"\n",
    "        scale = (2/self.input_dim)**0.5\n",
    "        rho_init = np.log(np.exp((2/self.input_dim)**0.5) - 1)\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-4, -3))\n",
    "        \n",
    "    def forward(self, x, sample = True):\n",
    "        \n",
    "        if sample:\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "            bias_epsilons =  Variable(self.bias_mus.data.new(self.bias_mus.size()).normal_())\n",
    "            \n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            bias_stds = torch.log(1 + torch.exp(self.bias_rhos))\n",
    "            \n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            \"mu-mean / rho-std of weight and bias\"\n",
    "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            bias_sample = self.bias_mus + bias_epsilons*bias_stds\n",
    "            \n",
    "            output = torch.mm(x, weight_sample) + bias_sample\n",
    "            \n",
    "            # computing the KL loss term\n",
    "            \"This uses a known equation of KL divergence between two gaussians\"\n",
    "            \"Since it is element-wise independent gaussian for both prior&posterior, we sums over them\"\n",
    "            prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "            KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "            KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "            prior_cov, varpost_cov = self.prior.sigma**2, bias_stds**2\n",
    "            KL_loss = KL_loss + 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*bias_stds.numel()\n",
    "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "            KL_loss = KL_loss + 0.5*((self.bias_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "            return output, KL_loss\n",
    "        \n",
    "        else:\n",
    "            output = torch.mm(x, self.weight_mus) + self.bias_mus\n",
    "            return output, KL_loss\n",
    "        \n",
    "    # return list elements\n",
    "    def sample_layer(self, no_samples):\n",
    "        all_samples = []\n",
    "        for i in range(no_samples):\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "            \n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            \n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "            # convert posterior samples to flatten list\n",
    "            all_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n",
    "            \n",
    "        return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBP_Heteroscedastic_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units):\n",
    "        super(BBP_Heteroscedastic_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units, gaussian(0, 1))\n",
    "        \"The BNN output fits the element-wise mean/std of the data. Thus 2*output_dim is the shape we need\"\n",
    "        \"Note that the BNN output needs not to be the actually data's simulation here\"\n",
    "        \"heteroscedastic only means we assume element-wise variance of data can be different\"\n",
    "        self.layer2 = BayesLinear_Normalq(num_units, 2*output_dim, gaussian(0, 1))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        \n",
    "        return x, KL_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBP_Heteroscedastic_Model_Wrapper:\n",
    "    \n",
    "    def __init__(self, network, learn_rate, batch_size, no_batches):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "        \n",
    "        self.network = network\n",
    "        #self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "    \n",
    "    def fit(self, x, y, no_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=False)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        \n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss_total = self.network(x)\n",
    "\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            \"this one is designed spefically for output_dim=1, therefore :1/1: is used here\"\n",
    "            \"this is actually the log likelihood of data, under posterior distributed theta\"\n",
    "            fit_loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        \"We deaks with ONLY ONE BATCH of data here, thus the scaling on KL_divergence is 1/no_batches\"\n",
    "        KL_loss_total = KL_loss_total/self.no_batches\n",
    "        \"I do not think further scaling on KLloss with no_samples(MC samples) is correct\"\n",
    "        \"it is not a MC estimate, unlike fit_loss\"\n",
    "        total_loss = (fit_loss_total)/(no_samples*x.shape[0]) + KL_loss_total/x.shape[0]\n",
    "        #total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss_total/no_samples, KL_loss_total\n",
    "    \n",
    "    \n",
    "    def get_loss_and_rmse(self, x, y, no_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=False)\n",
    "        \n",
    "        means, stds = [], []\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss_total = self.network(x)\n",
    "            means.append(output[:, :1, None])\n",
    "            stds.append(output[:, 1:, None].exp())\n",
    "            \n",
    "        means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
    "        mean = means.mean(dim=2)\n",
    "        std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
    "            \n",
    "        # calculate fit loss based on mean and standard deviation of output\n",
    "        logliks = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1, sum_reduce=False)\n",
    "        rmse = float((((mean - y)**2).mean()**0.5).cpu().data)\n",
    "\n",
    "        return logliks, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     1/ 2000, Fit loss = 355.464, KL loss = 2121.073\n",
      "Epoch:   101/ 2000, Fit loss = 355.314, KL loss = 1365.720\n",
      "Epoch:   201/ 2000, Fit loss = 390.397, KL loss =  794.844\n",
      "Epoch:   301/ 2000, Fit loss = 392.675, KL loss =  569.122\n",
      "Epoch:   401/ 2000, Fit loss = 410.755, KL loss =  508.549\n",
      "Epoch:   501/ 2000, Fit loss = 473.650, KL loss =  478.712\n",
      "Epoch:   601/ 2000, Fit loss = 397.699, KL loss =  466.429\n",
      "Epoch:   701/ 2000, Fit loss = 391.831, KL loss =  458.684\n",
      "Epoch:   801/ 2000, Fit loss = 403.311, KL loss =  453.966\n",
      "Epoch:   901/ 2000, Fit loss = 385.053, KL loss =  456.998\n",
      "Epoch:  1001/ 2000, Fit loss = 413.837, KL loss =  449.025\n",
      "Epoch:  1101/ 2000, Fit loss = 390.200, KL loss =  457.307\n",
      "Epoch:  1201/ 2000, Fit loss = 382.766, KL loss =  461.157\n",
      "Epoch:  1301/ 2000, Fit loss = 413.057, KL loss =  456.949\n",
      "Epoch:  1401/ 2000, Fit loss = 514.760, KL loss =  452.439\n",
      "Epoch:  1501/ 2000, Fit loss = 408.611, KL loss =  461.020\n",
      "Epoch:  1601/ 2000, Fit loss = 397.922, KL loss =  458.716\n",
      "Epoch:  1701/ 2000, Fit loss = 382.204, KL loss =  455.154\n",
      "Epoch:  1801/ 2000, Fit loss = 382.650, KL loss =  449.233\n",
      "Epoch:  1901/ 2000, Fit loss = 394.670, KL loss =  451.432\n",
      "Epoch:  2000/ 2000, Fit loss = 397.751, KL loss =  455.139\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "no_points = 400\n",
    "lengthscale = 1\n",
    "variance = 1.0\n",
    "sig_noise = 0.3\n",
    "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
    "x.sort(axis = 0)\n",
    "\n",
    "\"blablabla just to get a covariance function used to generate datas\"\n",
    "C = np.diag(abs(np.random.randn(no_points))) + (np.eye(no_points)*(x + 2)**2*sig_noise**2)\n",
    "\n",
    "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
    "y = (y - y.mean())\n",
    "x_train = x[75:325]\n",
    "y_mean = y[75:325].mean()\n",
    "y_std = y[75:325].var()**0.5\n",
    "y_train = (y[75:325] - y_mean)/y_std\n",
    "\n",
    "\n",
    "num_epochs, batch_size, nb_train = 2000, len(x_train), len(x_train)\n",
    "\n",
    "net = BBP_Heteroscedastic_Model_Wrapper(network=BBP_Heteroscedastic_Model(input_dim=1, output_dim=1, num_units=200),\n",
    "                                        learn_rate=1e-2, batch_size=batch_size, no_batches=1)\n",
    "\n",
    "fit_loss_train = np.zeros(num_epochs)\n",
    "KL_loss_train = np.zeros(num_epochs)\n",
    "total_loss = np.zeros(num_epochs)\n",
    "\n",
    "best_net, best_loss = None, float('inf')\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    fit_loss, KL_loss = net.fit(x_train, y_train, no_samples = 10)\n",
    "    fit_loss_train[i] += fit_loss.cpu().data.numpy()\n",
    "    KL_loss_train[i] += KL_loss.cpu().data.numpy()\n",
    "    \n",
    "    total_loss[i] = fit_loss_train[i] + KL_loss_train[i]\n",
    "    \n",
    "    if fit_loss < best_loss:\n",
    "        best_loss = fit_loss\n",
    "        best_net = copy.deepcopy(net.network)\n",
    "        \n",
    "    if i % 100 == 0 or i == num_epochs - 1:\n",
    "        \n",
    "        print(\"Epoch: %5d/%5d, Fit loss = %7.3f, KL loss = %8.3f\" %\n",
    "              (i + 1, num_epochs, fit_loss_train[i], KL_loss_train[i]))\n",
    "\n",
    "        samples = []\n",
    "        for i in range(100):\n",
    "            preds = net.network.forward(torch.linspace(-3, 3, 200))[0]\n",
    "            samples.append(preds.cpu().data.numpy()[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
