{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.priors import *\n",
    "from src.base_net import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "tod = torch.distributions\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire structure of BNN with Bayes Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropic_gauss_loglike(x, mu, sigma, do_sum=True):\n",
    "    cte_term = -(0.5) * np.log(2 * np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu) / sigma\n",
    "    dist_term = -(0.5) * (inner ** 2)\n",
    "\n",
    "    if do_sum:\n",
    "        out = (cte_term + det_sig_term + dist_term).sum()  # sum over all weights\n",
    "    else:\n",
    "        out = (cte_term + det_sig_term + dist_term)\n",
    "    return out    \n",
    "\n",
    "\"Sample according to parameterized posterior distribution of weights\"\n",
    "\"posterior is assumed to be element-wise normal distribuiton for the weights\"\n",
    "def sample_weights(W_mu, b_mu, W_p, b_p):\n",
    "    \"Quick method for sampling weights and exporting weights\"\n",
    "    \".data.new is just to create same typed tensor of certain shape\"\n",
    "    \"Q1. what is the type of W_mu, and why is tensor type failed for the command .normal_()?\"\n",
    "    \"\"\"\n",
    "    A1. W_mu is a nn.Paramter which accepts .normal_(), this essentially create N(0,1) sampled values on\n",
    "    each of its entries\n",
    "    A2. W is (entry-wise) normal distributed sample with mean W_mu, (entry-wise) standard deviation std_W.\n",
    "    A3. W_p is used to model std_W, through a softplus function added with 1e-6.\n",
    "    A4. Similarly for b, too. Except that we can use no bias, which is the case when b_mu is None.\n",
    "    A5. A softplus function always keeps output positive, making std reasonable, at the same time, its input\n",
    "        which is the W_p & b_p is not constrained at all, which is why we use it.\n",
    "    \"\"\"\n",
    "    eps_W = W_mu.new(W_mu.shape).normal_()\n",
    "    std_W = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n",
    "    W = W_mu + 1 * std_W * eps_W\n",
    "    \n",
    "    if b_mu is not None:\n",
    "        std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n",
    "        eps_b = b_mu.new(b_mu.shape).normal_()        \n",
    "        b = b_mu + 1 * std_b * eps_b\n",
    "    else:\n",
    "        b = None\n",
    "        \n",
    "    return W, b\n",
    "\n",
    "\n",
    "class BayesLinear_Normalq(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Layer where weights are sampled from a fully \n",
    "    factorised Normal with learnable parameters. \n",
    "    The likelihood of the weight samples under the prior\n",
    "    and the approximate posterior are returned with each\n",
    "    forward pass in order to estimate the KL term in \n",
    "    the ELBO.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out, prior_class):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out= n_out\n",
    "        self.prior = prior_class\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.W_mu = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-0.1,0.1))\n",
    "        self.W_p = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-3,-2))\n",
    "        \n",
    "        self.b_mu = nn.Parameter(torch.Tensor(self.n_out).uniform_(-0.1, 0.1))\n",
    "        self.b_p = nn.Parameter(torch.Tensor(self.n_out).uniform_(-3, -2))\n",
    "        \n",
    "        self.lpw = 0\n",
    "        self.lqw = 0\n",
    "        \n",
    "    \"\"\"\n",
    "    X shape (batch_size, n_in)\n",
    "    \"\"\"    \n",
    "    def forward(self, X, sample=False):\n",
    "        \"the self.training is True by default so it doesn't really matter what sample is here\"\n",
    "        if not self.training and not sample:\n",
    "            \"Expand simply copies and broadcast along first axis to shape (batch_size, n_out)\"\n",
    "            output = X @ self.W_mu + self.b_mu.unsqueeze(0)\n",
    "            return output, 0, 0\n",
    "        else:\n",
    "            # the same random sample is used for every element in the minibatch\n",
    "            \"Source of randomness, in fact a Gaussian Noise Here\"\n",
    "            \"Note that the way we generate W & b is in align with the choice of the approximate posterior\"\n",
    "            \"And it has nothing to do with the prior\"\n",
    "            eps_W = self.W_mu.new(self.W_mu.shape).normal_()\n",
    "            eps_b = self.b_mu.new(self.b_mu.shape).normal_()\n",
    "            \n",
    "            # sample parameters\n",
    "            std_w = 1e-6 + F.softplus(self.W_p, beta=1, threshold=20)\n",
    "            std_b = 1e-6 + F.softplus(self.b_p, beta=1, threshold=20)\n",
    "            \n",
    "            W = self.W_mu + 1 * std_w * eps_W\n",
    "            b = self.b_mu + 1 * std_b * eps_b\n",
    "            \n",
    "            output = X @ W + b.unsqueeze(0) # (batch_size, n_output)\n",
    "            \"approximate posterior: isotropic_gauss_loglike is in the 'prior.py' file \"\n",
    "            lqw = isotropic_gauss_loglike(W, self.W_mu, std_w) + isotropic_gauss_loglike(b, self.b_mu, std_b)\n",
    "            \"log-likelihood of the parameters (weights and biases) under the prior\"\n",
    "            lpw = self.prior.loglike(W) + self.prior.loglike(b)\n",
    "            return output, lqw, lpw\n",
    "        \n",
    "class bayes_linear_2L(nn.Module):\n",
    "    \"\"\"2 hidden layer Bayes By Backprop (VI) Network\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, n_hid, prior_instance):\n",
    "        super(bayes_linear_2L, self).__init__()\n",
    "\n",
    "        # prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n",
    "        # prior_instance = spike_slab_2GMM(mu1=0, mu2=0, sigma1=0.135, sigma2=0.001, pi=0.5)\n",
    "        # prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n",
    "        self.prior_instance = prior_instance\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \"n_hid: input dim and output dim for the hidden layers (2nd layer)\"\n",
    "        self.bfc1 = BayesLinear_Normalq(input_dim, n_hid, self.prior_instance)\n",
    "        self.bfc2 = BayesLinear_Normalq(n_hid, n_hid, self.prior_instance)\n",
    "        self.bfc3 = BayesLinear_Normalq(n_hid, output_dim, self.prior_instance)\n",
    "\n",
    "        # choose your non linearity\n",
    "        # self.act = nn.Tanh()\n",
    "        # self.act = nn.Sigmoid()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        # self.act = nn.ELU(inplace=True)\n",
    "        # self.act = nn.SELU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, sample=False):\n",
    "        \"note that sample is just a bool type with True or False value\"\n",
    "        tlqw = 0\n",
    "        tlpw = 0\n",
    "        \"Q2. what does view do? The answer is with the type of input x\"\n",
    "        \"A2: this is the key which reshaped input into (batch_size, input_dim)\"\n",
    "        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n",
    "        # -----------------\n",
    "        \"Calling the Forward Pass of first layer, essentially\"\n",
    "        x, lqw, lpw = self.bfc1(x, sample)\n",
    "        tlqw = tlqw + lqw\n",
    "        tlpw = tlpw + lpw\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        x, lqw, lpw = self.bfc2(x, sample)\n",
    "        tlqw = tlqw + lqw\n",
    "        tlpw = tlpw + lpw\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y, lqw, lpw = self.bfc3(x, sample)\n",
    "        tlqw = tlqw + lqw\n",
    "        tlpw = tlpw + lpw\n",
    "\n",
    "        return y, tlqw, tlpw\n",
    "    \n",
    "    \n",
    "    def sample_predict(self, x, Nsamples):\n",
    "        \"\"\"\n",
    "        Not sure what it means: Used for estimating the data's likelihood by approximately marginalising the weights with MC\n",
    "        \n",
    "        Take a number (Nsamples) of samples of weight sets according to the posterior distribution assumption, compute\n",
    "        the output or predictions of the network based on those values of weights and biases.\n",
    "        \"\"\"\n",
    "        # Just copies type from x, initializes new vector\n",
    "        predictions = x.new(Nsamples, x.shape[0], self.output_dim)\n",
    "        tlqw_vec = np.zeros(Nsamples)\n",
    "        tlpw_vec = np.zeros(Nsamples)\n",
    "\n",
    "        for i in range(Nsamples):\n",
    "            y, tlqw, tlpw = self.forward(x, sample=True)\n",
    "            predictions[i] = y\n",
    "            tlqw_vec[i] = tlqw\n",
    "            tlpw_vec[i] = tlpw\n",
    "\n",
    "        return predictions, tlqw_vec, tlpw_vec\n",
    "    \n",
    "\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            \"essentially call the nn.Module.train() which sets the training mode for the model\"\n",
    "            self.model.train()\n",
    "        else:\n",
    "            \"set the nn.Module to eval mode\"\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch\n",
    "    \n",
    "    \n",
    "    \n",
    "class BBP_Bayes_Net(BaseNet):\n",
    "    \"\"\"\n",
    "    Full network wrapper for Bayes By Backprop nets with methods for training, \n",
    "    prediction and weight prunning\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, lr=1e-3, channels_in=5, side_in=1, cuda=True, classes=5, batch_size=5, Nbatches=1,\n",
    "                 nhid=10, prior_instance=laplace_prior(mu=0, b=0.1)):\n",
    "        super(BBP_Bayes_Net, self).__init__()\n",
    "        cprint('y', ' Creating Net!! ')\n",
    "        self.lr = lr\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.cuda = cuda\n",
    "        self.channels_in = channels_in\n",
    "        self.classes = classes\n",
    "        \"entire number of data is the product of batch_size and Nbatches\"\n",
    "        \"more like size of each minibatch\"\n",
    "        self.batch_size = batch_size\n",
    "        \"more like no. of minibatches\"\n",
    "        self.Nbatches = Nbatches\n",
    "        self.prior_instance = prior_instance\n",
    "        self.nhid = nhid\n",
    "        self.side_in = side_in\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.test = False\n",
    "    \n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "        \"Q3: why is the input dim equals the product and not just channels_in ?\"\n",
    "        \"A3: We input image data, which has side_length * side_length * no. layers(RGB) number of data points\"\n",
    "        self.model = bayes_linear_2L(input_dim=self.channels_in * self.side_in * self.side_in,\n",
    "                                     output_dim=self.classes, n_hid=self.nhid, prior_instance=self.prior_instance)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "        #             cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n",
    "        #                                           weight_decay=0)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0)\n",
    "\n",
    "    #         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n",
    "    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n",
    "    \n",
    "    def fit(self, x, y, samples=1):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        if samples == 1:\n",
    "            \"model used here is STOCHASTIC but with only 1 samples used\"\n",
    "            out, tlqw, tlpw = self.model(x)\n",
    "            \"\"\"\n",
    "            1. mean log probability of data given weights: mlpdw\n",
    "            2. out shape: (batch_size, out_dim), y shape: (batch_size)\n",
    "            3. This is a classification task, y takes value {1,2,...,out_dim}, out_dim is actually\n",
    "            the number of classes also. For the F.cross_entropy function, it first convert 'out',\n",
    "            whose value can be any real number, into a valid discrete distribution mass function,\n",
    "            then compute - log(p_{out}(y)), then summed over all the batches, which we can use\n",
    "            to construct MC estimate of Cross Entropy (Likelihood Cost) by taking average over batch\n",
    "            size.\n",
    "            \"\"\"\n",
    "            mlpdw = F.cross_entropy(out, y, reduction='sum')\n",
    "            \"expected KL divergence\"\n",
    "            \"Note that this term get scaled furthur by Nbatches\"\n",
    "            Edkl = (tlqw - tlpw) / self.Nbatches\n",
    "\n",
    "        elif samples > 1:\n",
    "            mlpdw_cum = 0\n",
    "            Edkl_cum = 0\n",
    "\n",
    "            for i in range(samples):\n",
    "                out, tlqw, tlpw = self.model(x, sample=True)\n",
    "                mlpdw_i = F.cross_entropy(out, y, reduction='sum')\n",
    "                Edkl_i = (tlqw - tlpw) / self.Nbatches\n",
    "                mlpdw_cum = mlpdw_cum + mlpdw_i\n",
    "                Edkl_cum = Edkl_cum + Edkl_i\n",
    "\n",
    "            mlpdw = mlpdw_cum / samples\n",
    "            Edkl = Edkl_cum / samples\n",
    "        \"loss function we wish to minimize, negative ELBO\"\n",
    "        loss = Edkl + mlpdw\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return Edkl.data, mlpdw.data, err\n",
    "\n",
    "    \n",
    "        \n",
    "    def eval(self, x, y, train=False):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out, _, _ = self.model(x)\n",
    "\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def sample_eval(self, x, y, Nsamples, logits=True, train=False):\n",
    "        \"\"\"Prediction, only returining result with weights marginalised\"\"\"\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out, _, _ = self.model.sample_predict(x, Nsamples)\n",
    "\n",
    "        \"\"\"\n",
    "        1. Cross-entropy in torch is basically a softmax (to get estimate prob) + \n",
    "           negative log-likelihood loss\n",
    "        2. NLLLoss in torch assumes the input to be already log-scale\n",
    "        3. Considering above two points, the main difference for the if/else below is that\n",
    "           we take mean over samples and softmanx to get prob or we take probs for each sample\n",
    "           and then average to get mean probs\n",
    "        \"\"\"\n",
    "        \n",
    "        if logits:\n",
    "            mean_out = out.mean(dim=0, keepdim=False)\n",
    "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
    "\n",
    "        else:\n",
    "            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "            probs = mean_out.data.cpu()\n",
    "\n",
    "            log_mean_probs_out = torch.log(mean_out)\n",
    "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def all_sample_eval(self, x, y, Nsamples):\n",
    "        \"\"\"Returns predictions for each MC sample\"\"\"\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out, _, _ = self.model.sample_predict(x, Nsamples)\n",
    "\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "        prob_out = prob_out.data\n",
    "\n",
    "        return prob_out\n",
    "\n",
    "    \"samples weights, flatten and record it but not the bias\"\n",
    "    def get_weight_samples(self, Nsamples=10):\n",
    "        \n",
    "        state_dict = self.model.state_dict()\n",
    "        weight_vec = []\n",
    "        Nsamples=10\n",
    "\n",
    "        for i in range(Nsamples):\n",
    "            for key in state_dict.keys():\n",
    "                \"each key for loop is a 'str' class object\"\n",
    "                \"the split('.') split up the str object according to position of '.' \" \n",
    "                \"and divide into list, weight_name is 'W_mu', 'W_p', 'b_mu' and 'b_p' \"\n",
    "                weight_dict = {}\n",
    "                weight_name = key.split('.')[1]\n",
    "                weight_dict[weight_name] = state_dict[key].cpu().data\n",
    "\n",
    "                if weight_name == 'b_p':\n",
    "                    W, b = sample_weights(W_mu=W_mu, b_mu=b_mu, W_p=W_p, b_p=b_p)\n",
    "                    \n",
    "                    for weight in W.cpu().view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)\n",
    "    \n",
    "    \"\"\"\n",
    "    Record here the value of absolute value of mean divided by std for weights (elementwise)\n",
    "    posterior distribuiton, probably useful for reparameterization\n",
    "\n",
    "    1. With thresh, then present element-wise whether threshold is exceeded\n",
    "    2. Without thresh, report the element-wise _SNR value\n",
    "    \"\"\"\n",
    "    def get_weight_SNR(self, thresh=None):\n",
    "        state_dict = self.model.state_dict()\n",
    "        weight_SNR_vec = []\n",
    "\n",
    "        if thresh is not None:\n",
    "            mask_dict = {}\n",
    "        \n",
    "        weight_dict = {}\n",
    "        for key in state_dict.keys():\n",
    "            weight_name = key.split('.')[1]\n",
    "            layer_name = key.split('.')[0]\n",
    "            weight_dict[weight_name] = state_dict[key].data\n",
    "            if weight_name == 'b_p':\n",
    "                W_mu, W_p, b_mu, b_p = weight_dict.values()\n",
    "                \"compute elementwise posterior std\"\n",
    "                sig_W = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n",
    "                sig_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n",
    "                \"element-wise posterior absolute mean divided by std\"\n",
    "                W_snr = (torch.abs(W_mu) / sig_W)\n",
    "                b_snr = (torch.abs(b_mu) / sig_b)\n",
    "                \"if thresh exist, return element-wise True/False: whether _snr > thresh\" \n",
    "                if thresh is not None:\n",
    "                    mask_dict[layer_name + '.W'] = (W_snr > thresh)\n",
    "                    mask_dict[layer_name + '.b'] = (b_snr > thresh)\n",
    "                \"if no thresh, record the _snr value\"\n",
    "                if thresh is None:\n",
    "                    for weight_SNR in W_snr.cpu().view(-1):\n",
    "                        weight_SNR_vec.append(weight_SNR)\n",
    "\n",
    "                    for weight_SNR in b_snr.cpu().view(-1):\n",
    "                        weight_SNR_vec.append(weight_SNR)\n",
    "\n",
    "        if thresh is not None:\n",
    "            return mask_dict\n",
    "        else:\n",
    "            return np.array(weight_SNR_vec)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Sample independetly Nsamples of weights and compute element-wise KL divergence \n",
    "    between approximate posterior and the prior distributions\n",
    "    1. With thresh, then present element-wise whether threshold is exceeded\n",
    "    2. Without thresh, report the element-wise KL divergence value\n",
    "    \"\"\"    \n",
    "    def get_weight_KLD(self, Nsamples=20, thresh=None):\n",
    "        state_dict = self.model.state_dict()\n",
    "        weight_KLD_vec = []\n",
    "\n",
    "        if thresh is not None:\n",
    "            mask_dict = {}\n",
    "\n",
    "        weight_dict = {}\n",
    "        for key in state_dict.keys():\n",
    "            weight_name = key.split('.')[1]\n",
    "            layer_name = key.split('.')[0]\n",
    "            weight_dict[weight_name] = state_dict[key].data\n",
    "            if weight_name == 'b_p':\n",
    "                W_mu, W_p, b_mu, b_p = weight_dict.values()\n",
    "                \"compute elementwise posterior std\"\n",
    "                std_W = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n",
    "                std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n",
    "\n",
    "                KL_W = W_mu.new(W_mu.shape).zero_()\n",
    "                KL_b = b_mu.new(b_mu.shape).zero_()\n",
    "\n",
    "                for i in range(Nsamples):\n",
    "                    W, b = sample_weights(W_mu=W_mu, b_mu=b_mu, W_p=W_p, b_p=b_p)\n",
    "                    # Note that this will currently not work with slab and spike prior\n",
    "                    \"posterior element-wise log like minus prior element-wise log like\"\n",
    "                    KL_W += isotropic_gauss_loglike(W, W_mu, std_W,\n",
    "                                                    do_sum=False) - self.model.prior_instance.loglike(W,\n",
    "                                                                                                      do_sum=False)\n",
    "                    \"posterior element-wise log like minus prior element-wise log like\"\n",
    "                    KL_b += isotropic_gauss_loglike(b, b_mu, std_b,\n",
    "                                                    do_sum=False) - self.model.prior_instance.loglike(b,\n",
    "                                                                                                      do_sum=False)\n",
    "                \"average over number of samples\"\n",
    "                KL_W /= Nsamples\n",
    "                KL_b /= Nsamples\n",
    "\n",
    "                \"thresh here is for the KL divergence value specifically\"\n",
    "                if thresh is not None:\n",
    "                    mask_dict[layer_name + '.W'] = KL_W > thresh\n",
    "                    mask_dict[layer_name + '.b'] = KL_b > thresh\n",
    "\n",
    "                else:\n",
    "\n",
    "                    for weight_KLD in KL_W.cpu().view(-1):\n",
    "                        weight_KLD_vec.append(weight_KLD)\n",
    "\n",
    "                    for weight_KLD in KL_b.cpu().view(-1):\n",
    "                        weight_KLD_vec.append(weight_KLD)\n",
    "    \n",
    "\n",
    "        if thresh is not None:\n",
    "            return mask_dict\n",
    "        else:\n",
    "            return np.array(weight_KLD_vec)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Masking the model's parameter, if threshold is not exceeded, reset the mean/std for\n",
    "    posterior distribuiton for the weights to zero/~0.01.\n",
    "    \"\"\"\n",
    "    def mask_model(self, Nsamples=0, thresh=0):\n",
    "        '''\n",
    "        Nsamples is used to select SNR (0) or KLD (>0) based masking\n",
    "        '''\n",
    "        original_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "        \"Note that = means that changing value of state_dict will also change values for RHS\"\n",
    "        state_dict = self.model.state_dict()\n",
    "\n",
    "        if Nsamples == 0:\n",
    "            mask_dict = self.get_weight_SNR(thresh=thresh)\n",
    "        else:\n",
    "            mask_dict = self.get_weight_KLD(Nsamples=Nsamples, thresh=thresh)\n",
    "\n",
    "        n_unmasked = 0\n",
    "\n",
    "        previous_layer_name = ''\n",
    "        for key in state_dict.keys():\n",
    "            layer_name = key.split('.')[0]\n",
    "            if layer_name != previous_layer_name:\n",
    "                previous_layer_name = layer_name\n",
    "                \"if element value below threshold, reset to mean zero and small std (~0.01)\"\n",
    "                \"this procedure is called masking, put a maks on these values\"\n",
    "                state_dict[layer_name + '.W_mu'][~mask_dict[layer_name + '.W']] = 0\n",
    "                state_dict[layer_name + '.W_p'][~mask_dict[layer_name + '.W']] = -1000\n",
    "                state_dict[layer_name + '.b_mu'][~mask_dict[layer_name + '.b']] = 0\n",
    "                state_dict[layer_name + '.b_p'][~mask_dict[layer_name + '.b']] = -1000\n",
    "                \"number of un-masked weight values\"\n",
    "                n_unmasked += mask_dict[layer_name + '.W'].sum()\n",
    "                n_unmasked += mask_dict[layer_name + '.b'].sum()\n",
    "\n",
    "        return original_state_dict, n_unmasked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = 'test/models_weight_uncertainty_MC_MNIST_gaussian'\n",
    "results_dir = 'test/results_weight_uncertainty_MC_MNIST_gaussian'\n",
    "\n",
    "\"\"\"\n",
    "create folder(s) in current location named with element in paths, \n",
    "after converting to list object\n",
    "\"\"\"\n",
    "def mkdir(paths):\n",
    "    \"if not already a list, make it a list\"\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    \"\"\n",
    "    for path in paths:\n",
    "        \"if not a directory, make it a directory\"\n",
    "        if not os.path.isdir(path):\n",
    "            \"creates a folder named path in the current location\"\n",
    "            os.makedirs(path)\n",
    "            \n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train config\n",
    "NTrainPointsMNIST = 60000\n",
    "batch_size = 100\n",
    "nb_epochs = 160\n",
    "log_interval = 1\n",
    "\n",
    "savemodel_its = [20, 50, 80, 120]\n",
    "save_dicts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is comprised of 70,000 handwritten numeric digit images and their respective labels.\n",
    "\n",
    "There are 60,000 training images and 10,000 test images, all of which are 28 pixels by 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "\"FIX for data loading issue: 503\"\n",
    "\"seems to be a server issue which is fixed by using another amazon server link\"\n",
    "new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "datasets.MNIST.resources = [\n",
    "   ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "   for url, md5 in datasets.MNIST.resources\n",
    "]\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 0.02M\n"
     ]
    }
   ],
   "source": [
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-3\n",
    "nsamples = 3\n",
    "########################################################################################\n",
    "net = BBP_Bayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n",
    "          Nbatches=(NTrainPointsMNIST/batch_size))\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "# best_cost = np.inf\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "tic0 = time.time()\n",
    "\n",
    "\n",
    "for i in range(epoch, nb_epochs):\n",
    "    \n",
    "    \"the object class BaseNet grants the sub-object class BBP_Bayes_Net a .set_mode_train function\"\n",
    "    net.set_mode_train(True)\n",
    "\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        \".fit trains the model with one step of optimization\"\n",
    "        \"x shape: (batch_size, channels_in, side_in, side_in)\"\n",
    "        \"Q4: why is shape of x not (batch_size, input_dim) and still work?\" \n",
    "        \"A4: See A2, bayes_linear_2L explicitly reshapes the input\"\n",
    "        \"y shape: (batch_size)\"\n",
    "        \"nsamples is MC samples for weight used to evaluate loss/err, here set to 3\"\n",
    "        cost_dkl, cost_pred, err = net.fit(x, y, samples=nsamples)\n",
    "\n",
    "        err_train[i] += err\n",
    "        kl_cost_train[i] += cost_dkl\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        \"batch_size: len(x) / number of training points accumulate: nb_samples\"\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    kl_cost_train[i] /= nb_samples\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    \n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_KL = %f, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, kl_cost_train[i], pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "    \n",
    "    # Save state dict\n",
    "    \n",
    "    if i in savemodel_its:\n",
    "        save_dicts.append(copy.deepcopy(net.model.state_dict()))\n",
    "        \n",
    "    # ---- dev\n",
    "    \"compute the test error among validation data set, save the best model / final model\"\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "            \n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "net.save(models_dir+'/theta_last.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "\n",
    "\n",
    "## Save results for plots\n",
    "# np.save('results/test_predictions.npy', test_predictions)\n",
    "np.save(results_dir + '/cost_train.npy', kl_cost_train)\n",
    "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "\n",
    "textsize = 15\n",
    "marker=5\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(pred_cost_train, 'r--')\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], 'b-')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(kl_cost_train, 'r')\n",
    "ax1.set_ylabel('nats?')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax = plt.gca()\n",
    "plt.title('DKL (per sample)')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], 'b-')\n",
    "ax2.semilogy(100 * err_train, 'r--')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "    ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/err.png',  bbox_extra_artists=(lgd,), box_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
